#!/usr/bin/env python3

# Punctum Scraper, (c) Martin Keegan, Open Book Publishers, 2019.
# To the extent if any to which this code is subject to copyright, it is
# available under the Apache Licence v2.0

# This tool takes Punctum Books' metadata CSV file, and a TSV file from
# OAPEN; it creates a SQLite database in the same directory, mapping
# Punctum's book DOIs to the URLs for their cover images. This database
# is consulted in preference to scraping the OAPEN website, to save wear
# and tear on OAPEN's webserver.

# The OAPEN TSV file must be pre-processed if its first line (the column
# header names) is delimited with commas rather than tabs; additionally, the
# leading spaces should be removed from the field names [sic]. It may be
# obtained from http://oapen.org/content/metadata

# Usage:
#   scrape_cover_urls punctum_books_metadata.csv oapen.tsv

import csv
import sys
import requests
import database
from html.parser import HTMLParser

import punctum

cover_url_prefix = "https://www.oapen.org"

def get_oapen_mappings(path):
    """Return a lazy list of tuples of information from OAPEN's TSV file.
       This file has to have been pre-processed as described at the top of
       this python file.

       List of the form:
       (OAPEN ID, landing page URL),
       (OAPEN ID, landing page URL)
       ..."""
    oapencsv = csv.DictReader(open(path), delimiter='\t')
    for row in oapencsv:
        oapen_id = row['OAPEN_ID']
        oapen_references = row['OAPEN_URL']
        if oapen_references is None:
            continue
        landing_page_url = oapen_references.split("|")[0]
        yield oapen_id, landing_page_url

def get_doi_to_cover_url_mappings(path, oapen_landing_pages):
    """Return a lazy list of tuples, mapping a book's DOI to its landing
       page at OAPEN."""

    actual_books = (b for b in punctum.get_punctum_books(path)
                    if b is not None)
    for doi, oapen_id, book_data in actual_books:
        website = book_data['Website']
        oapen_landing_page = oapen_landing_pages[oapen_id]
        yield doi, oapen_landing_page

class OAPENHTMLParser(HTMLParser):
    """A toy HTML parser for the landing pages on OAPEN's website, e.g.,
    http://www.oapen.org/search?identifier=1004825

    It uses a trivial state machine to find the first <img> tag
    inside the tag <div class="cover">, and saves the <img> tag's
    src= attribute into self.found

    Doubtless there is a smoother way of doing this, but I couldn't be
    bothered to check if BeautifulSoup or the other decent parsers
    were shipped as standard with Python3."""

    right_element = False # True after the <div class="cover"> has been found
    found = None          # non-None, containing URL, once <img> has been found

    def handle_starttag(self, tag, attrs):
        if self.found is not None:
            return
        if tag not in ["div", "img"]:
            return
        aa = dict(attrs)
        if tag == "img" and self.right_element:
            assert "src" in aa
            self.found = aa["src"]
        if aa.get('class', None) == 'cover':
            self.right_element = True

    def handle_endtag(self, tag):
        return

    def handle_data(self, data):
        return

def find_cover_url(landing_page_url):
    """Given the URL of a landing page for a book, return an absolute URL
       for the book's cover image."""
    body = requests.get(landing_page_url).text
    parser = OAPENHTMLParser()
    parser.feed(body)
    return cover_url_prefix + parser.found

def run():
    try:
        _, publisher_csv, oapen_tsv = sys.argv
    except:
        print("Usage: scrape_cover_urls punctum_books_metadata.csv oapen.tsv",
              file=sys.stderr)
        exit(1)

    oapen_landing_pages = dict([ (k,v) for k,v in
                                 get_oapen_mappings(oapen_tsv) ])

    doi_to_url = dict([ (k,v) for k, v in 
                        get_doi_to_cover_url_mappings(publisher_csv,
                                                      oapen_landing_pages) ])
    db = database.get_cache_db_handle()

    for doi, oapen_landing_page_url in doi_to_url.items():
        print(doi, oapen_landing_page_url, file=sys.stderr)
        url = database.cached(db, doi)
        if url is not None:
            print("CACHED: ", url, file=sys.stderr)
        else:
            url = find_cover_url(oapen_landing_page_url)
            database.save_to_cache(db, doi, url)

if __name__ == '__main__':
    run()
