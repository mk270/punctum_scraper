#!/usr/bin/env python3

# (c) Martin Keegan, Open Book Publishers, 2019.
# To the extent if any to which this code is subject to copyright, it is
# available under the Apache Licence v2.0

# This tool takes Punctum Books' metadata CSV file, and a TSV file from
# OAPEN; it creates a SQLite database in the same directory, mapping
# Punctum's book DOIs to the URLs for their cover images. This database
# is consulted in preference to scraping the OAPEN website, to save wear
# and tear on OAPEN's webserver.

# The OAPEN TSV file must be pre-processed if its first line (the column
# header names) is delimited with commas rather than tabs; additionally, the
# leading spaces should be removed from the field names [sic]. It may be
# obtained from http://oapen.org/content/metadata

# Usage:
#   scrape_cover_urls punctum_books_metadata.csv oapen.tsv

import os
import csv
import sys
import urllib.parse as urlparse
import requests
from sqlite3 import dbapi2 as sqlite
from html.parser import HTMLParser

cover_url_prefix = "https://www.oapen.org"

sqlite_db_path = os.path.join(os.path.dirname(__file__),
                              'punctum_cover_cache.db')

def get_cache_db_handle():
    """Return a database handle to the DB; create the DB and initialise
       it with a schema if the file does not already exist."""
    create = not os.path.exists(sqlite_db_path)
    db = sqlite.connect(sqlite_db_path)
    if create:
        c = db.cursor()
        c.execute('''create table cover (doi text unique not null,
                                         cover_url text not null);''')
        db.commit()
    return db

def get_oapen_id(url):
    """Given a URL for a book on OAPEN, extract its OAPEN resource id."""
    assert "oapen.org" in url
    assert "http" in url
    parsed = urlparse.urlparse(url)
    return urlparse.parse_qs(parsed.query)['docid'][0]

def get_punctum_books(path):
    """Return a lazy list of tuples of information about Punctum's books,
       given a CSV file of their metadata, in the form:

       (DOI, OAPEN ID, { arbitrary stuff from CSV file }),
       (DOI, OAPEN ID, { arbitrary stuff from CSV file }),
       (DOI, OAPEN ID, { arbitrary stuff from CSV file }),
       ..."""
    puncsv = csv.DictReader(open(path))
    for row in puncsv:
        doi = row['DOI'].strip("\n")
        doc_type = row['Type of Document']
        oapen_url = row['OAPEN URL']
        assert doc_type in ['Book', 'Journal']
        if doc_type != 'Book':
            continue
        if "oapen.org" not in oapen_url:
            continue
        oapen_id = get_oapen_id(oapen_url)
        yield doi, oapen_id, row

def get_oapen_mappings(path):
    """Return a lazy list of tuples of information from OAPEN's TSV file.
       This file has to have been pre-processed as described at the top of
       this python file.

       List of the form:
       (OAPEN ID, landing page URL),
       (OAPEN ID, landing page URL)
       ..."""
    oapencsv = csv.DictReader(open(path), delimiter='\t')
    for row in oapencsv:
        oapen_id = row['OAPEN_ID']
        oapen_references = row['OAPEN_URL']
        if oapen_references is None:
            continue
        landing_page_url = oapen_references.split("|")[0]
        yield oapen_id, landing_page_url

def get_doi_to_cover_url_mappings(path, oapen_landing_pages):
    """Return a lazy list of tuples, mapping a book's DOI to its landing
       page at OAPEN."""
    for doi, oapen_id, book_data in get_punctum_books(path):
        website = book_data['Website']
        oapen_landing_page = oapen_landing_pages[oapen_id]
        yield doi, oapen_landing_page

class OAPENHTMLParser(HTMLParser):
    """A toy HTML parser for the landing pages on OAPEN's website, e.g.,
    http://www.oapen.org/search?identifier=1004825

    It uses a trivial state machine to find the first <img> tag
    inside the tag <div class="cover">, and saves the <img> tag's
    src= attribute into self.found

    Doubtless there is a smoother way of doing this, but I couldn't be
    bothered to check if BeautifulSoup or the other decent parsers
    were shipped as standard with Python3."""

    right_element = False # True after the <div class="cover"> has been found
    found = None          # non-None, containing URL, once <img> has been found

    def handle_starttag(self, tag, attrs):
        if self.found is not None:
            return
        if tag not in ["div", "img"]:
            return
        aa = dict(attrs)
        if tag == "img" and self.right_element:
            assert "src" in aa
            self.found = aa["src"]
        if aa.get('class', None) == 'cover':
            self.right_element = True

    def handle_endtag(self, tag):
        return

    def handle_data(self, data):
        return

def find_cover_url(landing_page_url):
    """Given the URL of a landing page for a book, return an absolute URL
       for the book's cover image."""
    body = requests.get(landing_page_url).text
    parser = OAPENHTMLParser()
    parser.feed(body)
    return cover_url_prefix + parser.found

def run():
    try:
        _, publisher_csv, oapen_tsv = sys.argv
    except:
        print("Usage: scrape_cover_urls punctum_books_metadata.csv oapen.tsv",
              file=sys.stderr)
        exit(1)

    oapen_landing_pages = dict([ (k,v) for k,v in
                                 get_oapen_mappings(oapen_tsv) ])

    doi_to_url = dict([ (k,v) for k, v in 
                        get_doi_to_cover_url_mappings(publisher_csv,
                                                      oapen_landing_pages) ])
    db = get_cache_db_handle()

    def cached(doi):
        c = db.cursor()
        args = (doi, )
        c.execute('''select cover_url from cover where doi = ?;''', args)
        res = c.fetchone()
        c.close()
        if res is None:
            return None
        return res[0]

    def save_to_cache(doi, url):
        c = db.cursor()
        args = (doi, url)
        c.execute('''insert into cover (doi, cover_url) values (?, ?);''', args)
        db.commit()
        c.close()

    for doi, oapen_landing_page_url in doi_to_url.items():
        print(doi, oapen_landing_page_url)
        url = cached(doi)
        if url is not None:
            print("CACHED: ", url)
        else:
            url = find_cover_url(oapen_landing_page_url)
            save_to_cache(doi, url)

if __name__ == '__main__':
    run()
